{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter3. VAE","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kTP3eBxDgDT5","colab_type":"text"},"source":["# VAE 구조\n","\n","![](https://user-images.githubusercontent.com/24144491/50323466-18d03700-051d-11e9-82ed-afb1b6e2666a.png)\n","\n","이 구조에서 input 그림이 있을 때 어떤 의미를 가진 구조를 거쳐 output이 나오게 되는지 3 단계로 나누어 살펴볼 것이다.\n","\n","⏳ **단계별 과정**\n","1. input: x –> 𝑞_∅ (𝑥)–> 𝜇_𝑖,𝜎_𝑖\n","2. 𝜇_𝑖, 𝜎_𝑖, 𝜖_𝑖 –> 𝑧_𝑖\n","3. 𝑧_𝑖 –> 𝑔_𝜃 (𝑧_𝑖) –> 𝑝_𝑖 : output"]},{"cell_type":"markdown","metadata":{"id":"9oKsT2cab8ik","colab_type":"text"},"source":["## sudo of implementations of a VAE\n","z_mean, z_log_variance = encoder(input_img)\n","\n","z = z_mean + exp(z_log_variance) * epsilon\n","\n","reconstructed_img = decode(z)\n","\n","model = Model(input_img, reconstructed_img)"]},{"cell_type":"code","metadata":{"id":"qPG9xOepX-a8","colab_type":"code","colab":{}},"source":["# VAE encoder network\n","\n","# import libraries\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","import keras\n","from keras import layers\n","from keras import backend as K\n","from keras.models import Model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p8dhIIvOdW2g","colab_type":"text"},"source":["# 1. Encoder\n","input: x –> 𝑞_∅ (𝑥)–> 𝜇_𝑖,𝜎_𝑖\n","\n","![](https://user-images.githubusercontent.com/24144491/50323467-1968cd80-051d-11e9-932c-a9b9ef91de58.png)\n","\n","* Input shape(x) : (28,28,1)\n","* 𝑞_∅ (𝑥) 는 encoder 함수인데, x가 주어졌을때(given) z값의 분포의 평균과 분산을 아웃풋으로 내는 함수이다.\n","> Encoder 함수의 output은 latent variable의 분포의 𝜇 와 𝜎 를 내고, 이 output값을 표현하는 확률밀도함수를 생각해볼 수 있다."]},{"cell_type":"code","metadata":{"id":"YqLAHsn0Y5Q2","colab_type":"code","colab":{}},"source":["img_shape = (28,28, 1)\n","batch_size = 16\n","latent_dim = 2\n","\n","encoder_input = keras.Input(shape = img_shape, name='Encoder_Input')\n","x = encoder_input\n","\n","x = layers.Conv2D(32, 3, padding = 'same', activation='relu')(x)\n","x = layers.Conv2D(64, 3, padding = 'same', activation='relu', strides=(2, 2))(x)\n","x = layers.Conv2D(64, 3, padding = 'same', activation='relu')(x)\n","x = layers.Conv2D(64, 3, padding = 'same', activation='relu')(x)\n","\n","# return tuple of integers of shape of x\n","# (14, 14, 64)\n","shape_before_flattening = K.int_shape(x)[1:]\n","\n","x = layers.Flatten()(x) # (None, 12544)\n","x = layers.Dense(32, activation='relu')(x) # (None, 32)\n","\n","z_mean = layers.Dense(latent_dim)(x)\n","z_log_var = layers.Dense(latent_dim)(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"166Yhcu9hQTO","colab_type":"text"},"source":["# 2. Reparameterization Trick (Sampling)\n","𝜇_𝑖, 𝜎_𝑖, 𝜖_𝑖 –> 𝑧_𝑖\n","\n","![](https://user-images.githubusercontent.com/24144491/50323468-1968cd80-051d-11e9-8031-ff19c4eaf7c8.png)\n","\n","sampling 과정이 없다면 encoder 결과에서 나온 값에 대한 decoder역시 한 값만 가지게 된다. 따라서 우리는 필연적으로 그 데이터의 확률분포와 같은 분포에서 하나를 뽑는 sampling을 해야한다. 하지만 그냥 sampling 한다면 sampling 한 값들을 backpropagation 할 수 없다.이를 해결하기 위해 reparmeterization trick을 사용한다고 한다. \n","\n","정규분포에서 z1를 샘플링하는 것이나, 입실론을 정규분포(자세히는 N(0,1))에서 샘플링하고 그 값을 분산과 곱하고 평균을 더해 z2를 만들거나 두 z1,z2 는 같은 분포를 가진다고 한다.\n","\n","그래서 **코드에서 epsilon을 먼저 정규분포에서 random하게 뽑고, 그 epsilon을 exp(z_log_var)과 곱하고 z_mean을 더한다. 그렇게 형성된 값이 z가 된다.**\n","\n",">latent variable에서 sample된 z라는 value (= decoder input)가 만들어진다."]},{"cell_type":"code","metadata":{"id":"5XI6hh8Eb-o2","colab_type":"code","colab":{}},"source":["# latent_space_sampling function\n","\n","def sampling(args):\n","  z_mean, z_log_var = args\n","  # 정규분포로부터의 난수값을 반환\n","  epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n","  return z_mean + K.exp(z_log_var) * epsilon\n","\n","z = layers.Lambda(sampling)([z_mean, z_log_var])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l2rqxFtikA_9","colab_type":"text"},"source":["# 3. Decoder\n","𝑧_𝑖 –> 𝑔_𝜃 (𝑧_𝑖) –> 𝑝_𝑖 : output\n","![](https://user-images.githubusercontent.com/24144491/50323470-1968cd80-051d-11e9-90b1-a2e69ddbcbdd.png)\n","z 값을 g 함수(decoder)에 넣고 deconv(Conv2DTranspose)를 해 원래 이미지 사이즈의 아웃풋 z_decoded가 나오게 된다. 이때 p_data(x)의 분포를 Bernoulli 로 가정했으므로 output 값은 0~1 사이 값을 가져야하고, 이를 위해 마지막 activatino function이 sigmoid로 설정되는 것이다."]},{"cell_type":"code","metadata":{"id":"PW4entcVj_jw","colab_type":"code","colab":{}},"source":["# VAE decoder network\n","# latent space의 포인트를 이미지로 맵핑\n","\n","decoder_input = layers.Input(K.int_shape(z)[1:]) # 잠재벡터의 차원(latent_dim) 과 같다\n","x = layers.Dense(np.prod(shape_before_flattening), activation = 'relu')(decoder_input) # np.prod(): 각 배열 요소를 곱함\n","x = layers.Reshape(shape_before_flattening)(x)\n","x = layers.Conv2DTranspose(32, 3, padding='same', activation='relu', strides=(2,2))(x)\n","x = layers.Conv2D(1, 3, padding='same', activation='sigmoid')(x)\n","\n","decoder_output = x\n","decoder = Model(inputs = decoder_input, outputs = decoder_output, name=\"Decoder\")\n","z_decoded = decoder(z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-e1GU_dKk9Zb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1595597158016,"user_tz":-540,"elapsed":486,"user":{"displayName":"지뇨뇽","photoUrl":"https://lh5.googleusercontent.com/-JblkEZU_9CA/AAAAAAAAAAI/AAAAAAAAA9E/pPczc2xYDZs/s64/photo.jpg","userId":"17511610036703028721"}},"outputId":"fc4f0c18-ed3a-471f-f5a8-306cb568731d"},"source":["z_decoded.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, None, None, 1])"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"BDs9xCK65Iyg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"status":"ok","timestamp":1595597158907,"user_tz":-540,"elapsed":486,"user":{"displayName":"지뇨뇽","photoUrl":"https://lh5.googleusercontent.com/-JblkEZU_9CA/AAAAAAAAAAI/AAAAAAAAA9E/pPczc2xYDZs/s64/photo.jpg","userId":"17511610036703028721"}},"outputId":"a5455d86-50a4-4c1a-a66a-2d141f5dbf96"},"source":["decoder.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"Decoder\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_23 (InputLayer)        (None, 2)                 0         \n","_________________________________________________________________\n","dense_47 (Dense)             (None, 12544)             37632     \n","_________________________________________________________________\n","reshape_11 (Reshape)         (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","conv2d_transpose_7 (Conv2DTr (None, 28, 28, 32)        18464     \n","_________________________________________________________________\n","conv2d_55 (Conv2D)           (None, 28, 28, 1)         289       \n","=================================================================\n","Total params: 56,385\n","Trainable params: 56,385\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vnR04KHS3F3k","colab_type":"text"},"source":["# VAE 학습\n","## Loss Function 이해\n","\n","Loss 는 크게 총 2가지 부분이 있다.\n","\n","![](https://user-images.githubusercontent.com/24144491/50323472-1a016400-051d-11e9-86b7-d8bf6a1a880f.png)\n","\n","* Reconstruction Loss(code에서는 xent_loss)\n","* Regularization Loss(code에서는 kl_loss)\n","\n","**Reconstruction Loss : X 와 New X와의 관계**\n","\n","디코더 부분의 pdf는 Bernoulli 분포를 따른다고 가정했으므로 그 둘간의 cross entropy를 구한다.\n","\n","**Regularization Loss : X의 분포와 근사한 분포의 차이**\n","\n","X가 원래 가지는 분포와 동일한 분포를 가지게 학습하게 하기위해 true 분포를 approximate 한 함수의 분포에 대한 loss term이 Regularization Loss다. 이때 loss는 true pdf 와 approximated pdf간의 D_kl(두 확률분포의 차이(거리))을 계산한다.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"TGLwNaYo0-lw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1595597160960,"user_tz":-540,"elapsed":971,"user":{"displayName":"지뇨뇽","photoUrl":"https://lh5.googleusercontent.com/-JblkEZU_9CA/AAAAAAAAAAI/AAAAAAAAA9E/pPczc2xYDZs/s64/photo.jpg","userId":"17511610036703028721"}},"outputId":"8431cfed-ce2f-4a54-b3bf-2785dcea239a"},"source":["# Custom layer used to compute the VAE loss\n","\n","class CustomVariationalLayer(keras.layers.Layer):\n","\n","  def vae_loss(self, x, z_decoded):\n","    x = K.flatten(x)\n","    print(K.shape(x))\n","    z_decoded = K.flatten(z_decoded)\n","    print(K.shape(z_decoded))\n","    xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n","    kl_loss = -5e-4*K.mean(1+z_log_var-K.square(z_mean)-K.exp(z_log_var), axis=-1)\n","    return K.mean(xent_loss + kl_loss)\n","\n","  def call(self, inputs):\n","    x = inputs[0]\n","    z_decoded = inputs[1]\n","    loss = self.vae_loss(x,z_decoded)\n","    self.add_loss(loss, inputs=inputs)\n","    return x\n","\n","y = CustomVariationalLayer()([encoder_input, z_decoded]) # check\n","y"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tensor(\"custom_variational_layer_8/Shape:0\", shape=(1,), dtype=int32)\n","Tensor(\"custom_variational_layer_8/Shape_1:0\", shape=(1,), dtype=int32)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'custom_variational_layer_8/Identity:0' shape=(None, 28, 28, 1) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"zj_mbecd6Fq7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":709},"executionInfo":{"status":"ok","timestamp":1595597166291,"user_tz":-540,"elapsed":1228,"user":{"displayName":"지뇨뇽","photoUrl":"https://lh5.googleusercontent.com/-JblkEZU_9CA/AAAAAAAAAAI/AAAAAAAAA9E/pPczc2xYDZs/s64/photo.jpg","userId":"17511610036703028721"}},"outputId":"1944756b-d804-49e8-a3cb-d15f43263252"},"source":["# Training the VAE\n","\n","from keras.datasets import mnist\n","\n","vae = Model(encoder_input, y)\n","vae.compile(optimizer='rmsprop',loss=None) # Model.compile(loptimizer, loss, metrics)\n","vae.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_6\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","Encoder_Input (InputLayer)      (None, 28, 28, 1)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 28, 28, 32)   320         Encoder_Input[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 14, 14, 64)   18496       conv2d_51[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 14, 14, 64)   36928       conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 14, 14, 64)   36928       conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","flatten_12 (Flatten)            (None, 12544)        0           conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","dense_44 (Dense)                (None, 32)           401440      flatten_12[0][0]                 \n","__________________________________________________________________________________________________\n","dense_45 (Dense)                (None, 2)            66          dense_44[0][0]                   \n","__________________________________________________________________________________________________\n","dense_46 (Dense)                (None, 2)            66          dense_44[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 2)            0           dense_45[0][0]                   \n","                                                                 dense_46[0][0]                   \n","__________________________________________________________________________________________________\n","Decoder (Model)                 (None, 28, 28, 1)    56385       lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","custom_variational_layer_8 (Cus [(None, 28, 28, 1),  0           Encoder_Input[0][0]              \n","                                                                 Decoder[1][0]                    \n","==================================================================================================\n","Total params: 550,629\n","Trainable params: 550,629\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py:819: UserWarning: Output custom_variational_layer_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_8.\n","  'be expecting any data to be passed to {0}.'.format(name))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"B8gDxhq1-pOZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1595597188748,"user_tz":-540,"elapsed":800,"user":{"displayName":"지뇨뇽","photoUrl":"https://lh5.googleusercontent.com/-JblkEZU_9CA/AAAAAAAAAAI/AAAAAAAAA9E/pPczc2xYDZs/s64/photo.jpg","userId":"17511610036703028721"}},"outputId":"be9f44f6-7ac8-46f7-d86d-f96b35366675"},"source":["y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, 28, 28, 1])"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"d8xaPe43AaNe","colab_type":"text"},"source":["# MNIST data에 적용"]},{"cell_type":"code","metadata":{"id":"a6mTVF-hAOnt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1595597269546,"user_tz":-540,"elapsed":2317,"user":{"displayName":"지뇨뇽","photoUrl":"https://lh5.googleusercontent.com/-JblkEZU_9CA/AAAAAAAAAAI/AAAAAAAAA9E/pPczc2xYDZs/s64/photo.jpg","userId":"17511610036703028721"}},"outputId":"a6f596a7-faaf-4331-e38c-7317bc4e6c52"},"source":["(x_train,_),(x_test,y_test) = mnist.load_data()\n","\n","x_train = x_train.astype('float32') /255.\n","x_train = x_train.reshape(x_train.shape+(1,)) # check\n","x_test = x_test.astype('float32') /255.\n","x_test = x_test.reshape(x_test.shape+(1,))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G6_TzeZHAiB3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"status":"error","timestamp":1595597838715,"user_tz":-540,"elapsed":567099,"user":{"displayName":"지뇨뇽","photoUrl":"https://lh5.googleusercontent.com/-JblkEZU_9CA/AAAAAAAAAAI/AAAAAAAAA9E/pPczc2xYDZs/s64/photo.jpg","userId":"17511610036703028721"}},"outputId":"a37ff64f-5e0f-4478-f50f-06319933fb10"},"source":["vae.fit(x=x_train, y=None, shuffle=True, epochs=10, batch_size=batch_size,validation_data=(x_test,None))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 60000 samples, validate on 10000 samples\n","Epoch 1/10\n","59984/60000 [============================>.] - ETA: 0s - loss: 0.2091Epoch 2/10\n","35008/60000 [================>.............] - ETA: 2:27 - loss: 0.1918"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-3c235592438d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"IgKbmPQ4AjHv","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","from scipy.stats import norm\n","\n","n=15\n","digit_size = 28\n","figure = np.zeros((digit_size*n, digit_size*n))\n","grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n","grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n","\n","for i, yi in enumerate(grid_x):\n","  for j, xi in enumerate(grid_y):\n","    z_sample = np.array([[xi, yi]])\n","    z_sample = np.tile(z_sample,batch_size).reshape(batch_size, 2)\n","    x_decoded = decoder.predict(z_sample, batch_size = batch_size)\n","    digit = x_decoded[0].reshape(digit_size, digit_size)\n","    figure[i * digit_size : (i+1)*digit_size, j*digit_size : (j+1)*digit_size] = digit\n","\n","plt.figure(figsize=(10,10))\n","plt.imshow(figure, cmap = 'Greys_r')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7i6gNB4wDgET","colab_type":"text"},"source":["![](https://user-images.githubusercontent.com/24144491/50323704-289c4b00-051e-11e9-8dad-f32bad3c0583.PNG)\n","\n","(z1,z2)에서 z1 20개(0.05~0.95), z2 20개, 총 400개의 순서쌍의 xi,yi에서 sample을 뽑아 시각화한것이 오른쪽 그림이다.\n","\n","참고[https://taeu.github.io/paper/deeplearning-paper-vae/]"]},{"cell_type":"code","metadata":{"id":"cHXYgxH7DtA3","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}